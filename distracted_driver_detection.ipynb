{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "distracted-driver-detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fI_NFwIIjCJ"
      },
      "source": [
        "# Install Intel Distribution of OpenVINO Toolkit\n",
        "Run the below cell to install OpenVino Directly to Colab (Takes 1 min)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsU7FRfSxvnE"
      },
      "source": [
        "!wget https://apt.repos.intel.com/openvino/2020/GPG-PUB-KEY-INTEL-OPENVINO-2020\n",
        "!apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2020\n",
        "!touch /etc/apt/sources.list.d/intel-openvino-2020.list\n",
        "!echo \"deb https://apt.repos.intel.com/openvino/2020 all main\" >> /etc/apt/sources.list.d/intel-openvino-2020.list\n",
        "\n",
        "!apt update\n",
        "!apt install intel-openvino-dev-ubuntu18-2020.4.287\n",
        "\n",
        "!pip install test-generator==0.1.1\n",
        "!source /opt/intel/openvino/bin/setupvars.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xGlB0cyJ1lu"
      },
      "source": [
        "# Download and Extract Dataset and Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvbKyWmHBz76"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown --id 12RPFKynXFu_A8ETAuwti_Hq4yXXz0GCr\n",
        "!gdown --id 1sLTFb4Rcv5HnlyE2iLFEOKqDbK8vbJ1s\n",
        "!unzip \"/content/state-farm-distracted-driver-detection.zip\"\n",
        "!unzip \"/content/models.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24W7VGcKBDi"
      },
      "source": [
        "# Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "oAUt-ax-BiaD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frLVP4FEKKIB"
      },
      "source": [
        "# Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jY60SFRUOWk",
        "outputId": "977c71a6-7e39-44db-897b-b0536513a05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def _prepareData(path): \n",
        "    imgsList = []\n",
        "    labels = []\n",
        "    for directory in sorted(glob.glob(os.path.join(path, '*')), key = lambda k: k.split(\"/\")[-1]):\n",
        "            for imgs in glob.glob(os.path.join(directory,'*.jpg')):\n",
        "                img_cv = cv2.imread(imgs)\n",
        "                img_cv_r = cv2.resize(img_cv,(128,128))\n",
        "                imgsList.append(img_cv_r)\n",
        "                labels.append(int(directory.split(\"/\")[-1].replace('c','')))\n",
        "    \n",
        "    X_Train, X_Test, Y_Train, Y_Test =  train_test_split(imgsList,labels, test_size = 0.2)\n",
        "    Y_Train = tf.keras.utils.to_categorical(Y_Train, num_classes=10)\n",
        "    Y_Test = tf.keras.utils.to_categorical(Y_Test, num_classes=10)\n",
        "\n",
        "    return np.array(X_Train), np.array(X_Test), Y_Train, Y_Test\n",
        "\n",
        "#Paths\n",
        "pathTrain_Images = \"/content/imgs/train/\"\n",
        "pathPropagate_Images =  \"/content/imgs/test/\"\n",
        "\n",
        "#List of Images for Train and Test\n",
        "X_Train, X_Test, Y_Train, Y_Test = _prepareData(pathTrain_Images)\n",
        "\n",
        "print(\"Size X_Train: {}, Size Y_Train: {}\".format(len(X_Train),len(Y_Train)))\n",
        "print(\"Size X_Test: {}, Size Y_Test: {}\".format(len(X_Test),len(Y_Test)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size X_Train: 17939, Size Y_Train: 17939\n",
            "Size X_Test: 4485, Size Y_Test: 4485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoUK21D4KM4g"
      },
      "source": [
        "# Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_MGNulGXOUc",
        "outputId": "c58ecc2c-d4f7-40a9-9d6b-d7b740125611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.InputLayer(input_shape=(128, 128, 3)))\n",
        "model.add(keras.layers.Conv2D(filters=32,kernel_size=(5,5),strides = (1,1),padding='same',activation='relu',name='Conv_1'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size = (2,2),name = 'Pool_1'))\n",
        "model.add(keras.layers.Conv2D(filters = 64,kernel_size = (5,5),strides = (1,1),padding = 'same',activation = 'relu',name = 'Conv_2'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size = (2,2),name = 'Pool_2'))\n",
        "model.add(keras.layers.Conv2D(filters = 128,kernel_size = (5,5),strides = (1,1),padding = 'same',activation = 'relu',name = 'Conv_3'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size = (2,2),name = 'Pool_3'))\n",
        "model.add(keras.layers.Conv2D(filters = 256,kernel_size = (5,5),strides = (1,1),padding = 'same',activation = 'relu',name = 'Conv_4'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size = (2,2),name = 'Pool_4'))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(units=1024, activation='relu',name = 'fc_1'))\n",
        "model.add(keras.layers.Dropout(rate=0.2))\n",
        "model.add(keras.layers.Dense(units=512, activation='relu',name = 'fc_2'))\n",
        "model.add(keras.layers.Dense(units=10,activation='softmax',name = 'fc_3'))\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False), metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Conv_1 (Conv2D)              (None, 128, 128, 32)      2432      \n",
            "_________________________________________________________________\n",
            "Pool_1 (MaxPooling2D)        (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "Conv_2 (Conv2D)              (None, 64, 64, 64)        51264     \n",
            "_________________________________________________________________\n",
            "Pool_2 (MaxPooling2D)        (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "Conv_3 (Conv2D)              (None, 32, 32, 128)       204928    \n",
            "_________________________________________________________________\n",
            "Pool_3 (MaxPooling2D)        (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "Conv_4 (Conv2D)              (None, 16, 16, 256)       819456    \n",
            "_________________________________________________________________\n",
            "Pool_4 (MaxPooling2D)        (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "fc_1 (Dense)                 (None, 1024)              16778240  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "fc_2 (Dense)                 (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "fc_3 (Dense)                 (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 18,386,250\n",
            "Trainable params: 18,386,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqwXL_sUKPbC"
      },
      "source": [
        "# Training Model\n",
        "\n",
        "**It takes 30 mins to train Model,you can also skip this step and continue to inferencing as I have already trained and downloaded that model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpT1IJu0XQyv"
      },
      "source": [
        "history = model.fit(x = X_Train, y=Y_Train,epochs = 10, batch_size = 500, verbose = 1,validation_split=0.2)\n",
        "model.save(\"/content/models/driver_distraction.h5\")\n",
        "test_loss, test_acc = model.evaluate(X_Test, Y_Test, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40auim9YKRjZ"
      },
      "source": [
        "# Inferencing Model (Normal)\n",
        "\n",
        "It Evaluates first 50 Images from Test Dataset \n",
        "\n",
        "Average Time per Image : 0.0551 Seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kyzt9EJXex0"
      },
      "source": [
        "import time\n",
        "model.load_weights('/content/models/driver_distraction.h5')\n",
        "df = pd.DataFrame({'img':[],'c0':[], 'c1':[],'c2':[], 'c3':[], 'c4':[],'c5':[], 'c6':[], 'c7':[], 'c8':[], 'c9':[]})\n",
        "def submission(pathPropagate_Images,df):\n",
        "    cnt=0\n",
        "    actions=[\"Safe Driving\",\"Texting Right Hand\",\"Talking on Phone Right Hand\",\"Texting Left Hand\",\"Talking on Phone Left Hand\",\"Operating on Radio\",\"Drinking\",\"Reaching Behind\",\"Hair and Makeup\",\"Talking to Passenger\"]\n",
        "\n",
        "    for imgs in glob.glob(os.path.join(pathPropagate_Images,'*.jpg')):\n",
        "        img_cv = cv2.imread(imgs)\n",
        "        img_cv_r = cv2.resize(img_cv,(128,128))\n",
        "        img_cv_predict = np.reshape(img_cv_r,[1,128,128,3])\n",
        "        arr_predict = model.predict(img_cv_predict,batch_size = 1)\n",
        "        arr=arr_predict[0].tolist()\n",
        "        mx=max(arr)\n",
        "        ind=arr.index(mx)\n",
        "        print(imgs,actions[ind])\n",
        "        cnt+=1\n",
        "        if(cnt>50):\n",
        "          break\n",
        "start_time = time.time()\n",
        "pathPropagate_Images =  \"/content/imgs/test/\"\n",
        "submission(pathPropagate_Images,df)\n",
        "print(\"Total Time: \", time.time()-start_time)\n",
        "print(\"Average Time Per Image: \", (time.time()-start_time)/50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDOvQmWDK9x3"
      },
      "source": [
        "# Inferencing using OpenVino\n",
        "\n",
        "### Step 1 : Freezing Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoBOsE07kErU"
      },
      "source": [
        "from tensorflow.python.framework import graph_io\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.compat.v1 as tf\n",
        "#Using Tensorflow 1.0 due to Compatibility issues\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Clear any previous session.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Directory for Saving Model\n",
        "save_pb_dir = '/content/models'\n",
        "model_fname = '/content/models/driver_distraction.h5'\n",
        "\n",
        "def freeze_graph(graph, session, output, save_pb_dir='/content/models', save_pb_name='frozen_model.pb', save_pb_as_text=False):\n",
        "    with graph.as_default():\n",
        "        graphdef_inf = tf.compat.v1.graph_util.remove_training_nodes(graph.as_graph_def())\n",
        "        graphdef_frozen = tf.compat.v1.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
        "        graph_io.write_graph(graphdef_frozen, save_pb_dir, save_pb_name, as_text=save_pb_as_text)\n",
        "        return graphdef_frozen\n",
        "\n",
        "\n",
        "tf.keras.backend.set_learning_phase(0) \n",
        "\n",
        "model = load_model(model_fname)\n",
        "session=tf.keras.backend.get_session()\n",
        "INPUT_NODE = [t.op.name for t in model.inputs]\n",
        "OUTPUT_NODE = [t.op.name for t in model.outputs]\n",
        "print(INPUT_NODE, OUTPUT_NODE)\n",
        "frozen_graph = freeze_graph(session.graph, session, [out.op.name for out in model.outputs], save_pb_dir=save_pb_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DE2jnNqLEwC"
      },
      "source": [
        "### Step 2 : Converted Frozen Model to BIN and XML format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOCPlwMXxuNn"
      },
      "source": [
        "mo_tf_path = '/opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py'\n",
        "\n",
        "pb_file = '/content/models/frozen_model.pb'\n",
        "output_dir = '/content/models'\n",
        "img_height = 128\n",
        "input_shape = [1, img_height, img_height, 3]\n",
        "input_shape_str = str(input_shape).replace(' ', '')\n",
        "\n",
        "#Running Commands to run OPENVINO's Model Optimiser Converter 'mo_tf.py'\n",
        "!source /opt/intel/openvino/bin/setupvars.sh\n",
        "!python {mo_tf_path} --input_model {pb_file} --output_dir {output_dir} --input_shape {input_shape_str} --data_type FP16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh2oOwy6LMmb"
      },
      "source": [
        "### Step 3 : Running Inference (OpenVino)\n",
        "\n",
        "This is evaluated on first 50 images from Testing Data from Dataset\n",
        "\n",
        "Average Time per Image : 0.0258 Seconds\n",
        "\n",
        "**OpenVino Inference is working 2.1 Times Faster than Normal Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_A5JChbkEpF"
      },
      "source": [
        "!source /opt/intel/openvino/bin/setupvars.sh && \\\n",
        "python /content/models/inference.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMm3F-CJL20a"
      },
      "source": [
        "### Optional : Running OpenVino Inference on your Own Images\n",
        "\n",
        "First Upload multiple jpg files and it will automatically give Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F_q3nWR6P9M"
      },
      "source": [
        "print(\"Upload JPG Images to be inferred\")\n",
        "files.upload()\n",
        "!source /opt/intel/openvino/bin/setupvars.sh && \\\n",
        "python /content/models/inference_uploaded_images.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}